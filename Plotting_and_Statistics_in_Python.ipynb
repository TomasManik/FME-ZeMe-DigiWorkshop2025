{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdrMy8-XV80n"
      },
      "source": [
        "# Workshop FME ZeMe - 9$^{th}$ Dec. 2025\n",
        "**FME ZeMe - Zero Emission Metal Production**  \n",
        "*Norwegian Research Centre for Sustainable Metallurgy*\n",
        "## Plotting and Statistics in Python\n",
        "*Tomáš Mánik, IMA, NTNU*\n",
        "\n",
        "---\n",
        "\n",
        "### Learning Outcomes\n",
        "\n",
        "By the end of the workshop, participants will be able to:\n",
        "\n",
        "- Load, inspect, and filter data using Pandas.\n",
        "- Create and customize 2D and 3D plots to visualize data.\n",
        "- Apply basic smoothing and filtering to noisy data.\n",
        "- Fit and interpret linear and non-linear regression models in Python.\n",
        "- Explore variability using histograms and other distribution plots.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "### Content\n",
        "**1. Introduction to Pandas DataFrame & Simple Plotting**\n",
        "   - Dataset: *Ferroalloy Global Production*\n",
        "   - Covariance and Correlation\n",
        "   - Exploratory Data Analysis\n",
        "\n",
        "**2. Data filtering and differentiation**\n",
        "   - Dataset: *Solidification of Aluminium*\n",
        "   - Savitzky-Golay filter\n",
        "   - Butterworth filter\n",
        "\n",
        "**3. Linear and Non-linear regression**\n",
        "   - Dataset: *Arrhenius equation*\n",
        "   - Confidence and prediction band\n",
        "   - Multivariate regression\n",
        "\n",
        "**4. 3D visualization**\n",
        "   - Scatter and Surface plot\n",
        "   - Contour plot\n",
        "\n",
        "**5. Exploring Variability**\n",
        "   - Dataset: *Grain size, hardness, heat treatment of ferritic steel*\n",
        "   - Histograms\n",
        "   - Kernel density estimates (KDE) (smooth distribution estimates)\n",
        "   - Violin plots\n",
        "   - 2D-Histograms\n",
        "   - `pairplot` and `jointplot`"
      ],
      "metadata": {
        "id": "xrtCnbLr_6iv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### A practical things to remeber\n",
        "\n",
        "* any text after the `#` sign is a comment\n",
        "* for better navigation, every cell is numbered as #1, #2, ...\n",
        "* lines in each cell are numbered\n",
        "* press `SHIFT + ENTER` to run a cell\n",
        "* columnwise selection -> `SHIFT + ALT + drag the mouse straight down`"
      ],
      "metadata": {
        "id": "d5G_8e0nWHV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the libraries"
      ],
      "metadata": {
        "id": "yzIaGMu3XAux"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgmsalqJV80n"
      },
      "outputs": [],
      "source": [
        "#1\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my0VMHOiV80o"
      },
      "source": [
        "---\n",
        "# Part 1: Introduction to Pandas & Simple Plotting\n",
        "\n",
        "## 1.1   Ferroalloy Global Production\n",
        "\n",
        "World data on Ferroalloy Global Production can be found here   \n",
        "<https://datasource.kapsarc.org/explore/dataset/production-of-ferro-alloys>\n",
        "\n",
        "Export the csv file from the webpage above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9u6KRGLV80o"
      },
      "outputs": [],
      "source": [
        "#2\n",
        "# Our base URL for the input files for this Workshop\n",
        "base_url = \"https://raw.githubusercontent.com/TomasManik/FME-ZeMe-DigiWorkshop2025/refs/heads/main/data/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "# Load ferroalloy production data\n",
        "df = pd.read_csv(base_url + \"production-of-ferro-alloys.csv\", sep=';')"
      ],
      "metadata": {
        "id": "zbKaEr0_gMOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOa3gSONV80p"
      },
      "outputs": [],
      "source": [
        "#4\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPdReNvYV80p"
      },
      "outputs": [],
      "source": [
        "#5\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzCg_LdqV80p"
      },
      "outputs": [],
      "source": [
        "#6\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7\n",
        "df['Country'].unique()"
      ],
      "metadata": {
        "id": "GlbxMkjTX99G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEFTPwaMV80q"
      },
      "outputs": [],
      "source": [
        "# 8\n",
        "df[df['Country'] == 'USA']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrJ89ryNV80q"
      },
      "outputs": [],
      "source": [
        "#9\n",
        "df[(df['Country'] == 'USA') & (df['Sub-commodity'] == 'Ferro-manganese')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soc3k8tNV80r"
      },
      "outputs": [],
      "source": [
        "#10\n",
        "df2 = df[(df['Country'] == 'USA') & (df['Sub-commodity'] == 'Ferro-manganese')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bPOwrc7V80r"
      },
      "outputs": [],
      "source": [
        "#11\n",
        "df2['Year']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztAcTa_SV80r"
      },
      "outputs": [],
      "source": [
        "#12\n",
        "df2['Production']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOmgqko_V80r"
      },
      "outputs": [],
      "source": [
        "#13\n",
        "plt.plot(df2['Year'], df2['Production'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot production for several countries"
      ],
      "metadata": {
        "id": "S1T8JvclkOqU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPONQmp1V80r"
      },
      "outputs": [],
      "source": [
        "#14\n",
        "for country in ['USA', 'Norway', 'Slovakia']:\n",
        "    df3 = df[(df['Country'] == country) & (df['Sub-commodity'] == 'Ferro-manganese')]\n",
        "    plt.plot(df3['Year'], df3['Production'], 'o', label=country)\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Production (tons)')\n",
        "plt.title('FerroManganese Production')\n",
        "plt.grid(True)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot production for several countries AND two commodities"
      ],
      "metadata": {
        "id": "xV8uGbV_lIUn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg4chYg2V80r"
      },
      "outputs": [],
      "source": [
        "#15\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16,4))\n",
        "\n",
        "for country in ['USA', 'Norway', 'Slovakia']:\n",
        "    df4 = df[(df['Country'] == country) & (df['Sub-commodity'] == 'Ferro-manganese')]\n",
        "    ax[0].plot(df4['Year'], df4['Production'], 'o', label=country)\n",
        "\n",
        "    df4 = df[(df['Country'] == country) & (df['Sub-commodity'] == 'Ferro-silicon')]\n",
        "    ax[1].plot(df4['Year'], df4['Production'], 'o', label=country)\n",
        "\n",
        "# ax[0].set_xlabel('Year')\n",
        "# ax[0].set_ylabel('Production (tons)')\n",
        "# ax[0].set_title('Ferro-manganese Production')\n",
        "# ax[0].grid(True)\n",
        "# ax[0].legend()\n",
        "\n",
        "# ax[1].set_xlabel('Year')\n",
        "# ax[1].set_ylabel('Production (tons)')\n",
        "# ax[1].set_title('Ferro-silicon Production')\n",
        "# ax[1].grid(True)\n",
        "# ax[1].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9iwV8IDV80r"
      },
      "source": [
        "* adjust better units\n",
        "* fix common y-axis\n",
        "* add the line (need to sort the data)\n",
        "* add one more sub-commodity and redo plotting even more general"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5CM234oV80r"
      },
      "outputs": [],
      "source": [
        "#16\n",
        "countries = ['Italy', 'Norway', 'Spain']\n",
        "commodities = ['Ferro-manganese', 'Ferro-silicon', 'Ferro-silico-manganese']\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=len(commodities), figsize=(16,4), sharey=True)\n",
        "\n",
        "for i, commodity in enumerate(commodities):\n",
        "    for country in countries:\n",
        "        df5 = df[(df['Country'] == country) & (df['Sub-commodity'] == commodity)].sort_values(by='Year')\n",
        "        ax[i].plot(df5['Year'], df5['Production']/1000, '-', label=country)\n",
        "    ax[i].set_xlabel('Year')\n",
        "    ax[i].set_title(commodity)\n",
        "    ax[i].grid(True)\n",
        "    ax[i].legend()\n",
        "\n",
        "ax[0].set_ylabel('Production (ktons)')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad2olVChV80r"
      },
      "source": [
        "## 1.2 Covariance and Correlation\n",
        "\n",
        "* choose one commodity and five countries\n",
        "* calculate the covariance and correlation matrix\n",
        "* visualize it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLYFCMClV80r"
      },
      "source": [
        "#### Covariance\n",
        "\n",
        "Covariance measures how two variables change together. For variables $X$ and $Y$, the sample covariance is:\n",
        "\n",
        "$$\n",
        "\\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})\n",
        "$$\n",
        "\n",
        "\n",
        "where $ \\bar{X} $ and $ \\bar{Y} $ are sample means.\n",
        "\n",
        "A **covariance matrix** summarizes covariances between pairs of variables in a dataset as\n",
        "\n",
        "$$\n",
        "\\mathbf{Cov}(X) =\n",
        "\\begin{pmatrix}\n",
        "\\text{Cov}(X_1, X_1) & \\text{Cov}(X_1, X_2) & \\cdots & \\text{Cov}(X_1, X_n) \\\\\n",
        "\\text{Cov}(X_2, X_1) & \\text{Cov}(X_2, X_2) & \\cdots & \\text{Cov}(X_2, X_n) \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\text{Cov}(X_n, X_1) & \\text{Cov}(X_n, X_2) & \\cdots & \\text{Cov}(X_n, X_n) \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "##### Where Is the Covariance Matrix Typically Used?\n",
        "* is a measure for how strong the linear relationship between two variables is\n",
        "* one of the main ingredients for finding the linear \"fit\" - the regression line - in a closed form solution\n",
        "* estimates **redundancy** - if $X_1$ is a linear function of $X_2$, it does not add any new information\n",
        "* is basis for **PCA** (principal component analysis)\n",
        "* to do **dimensionality reduction** - belongs to **unsupervised machine learning**\n",
        "* is affected by variable scales/units - difficult to interpret of how strong the dependecy is\n",
        "\n",
        "#### Correlation\n",
        "\n",
        "The **correlation coefficient** $ \\rho_{X,Y} $ provides a normalized, dimensionless measure of linear association:\n",
        "\n",
        "$$\n",
        "\\rho_{X,Y} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n",
        "$$\n",
        "\n",
        "where $ \\sigma_X $ and $ \\sigma_Y $ are the standard deviations of $X$ and $Y$.\n",
        "\n",
        "A **correlation matrix** replaces each covariance with the corresponding correlation coefficient, ranging from -1 (perfect negative) to 1 (perfect positive correlation):\n",
        "\n",
        "$$\n",
        "\\mathbf{Corr}(X) =\n",
        "\\begin{pmatrix}\n",
        "\\rho_{X_1, X_1} & \\rho_{X_1, X_2} & \\cdots & \\rho_{X_1, X_n} \\\\\n",
        "\\rho_{X_2, X_1} & \\rho_{X_2, X_2} & \\cdots & \\rho_{X_2, X_n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\rho_{X_n, X_1} & \\rho_{X_n, X_2} & \\cdots & \\rho_{X_n, X_n} \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "* correlation is unitless (normalized)\n",
        "* the matrix is symmetric (as well as the covariance matrix)\n",
        "* +1  means perfect positive linear correlation\n",
        "* -1  means perfect negative linear correlation\n",
        "* 0  variables are uncorrelated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqAlbuKnV80s"
      },
      "outputs": [],
      "source": [
        "#17\n",
        "countries = [\"India\", \"Italy\", \"Mexico\", \"Norway\", \"Spain\"]\n",
        "df1 = df[(df['Country'].isin(countries)) & (df['Sub-commodity'] == 'Ferro-silico-manganese')]\n",
        "df1 = df1[['Country','Year', 'Production']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81zyZt_RV80s"
      },
      "outputs": [],
      "source": [
        "#18\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnnJJOkcV80s"
      },
      "outputs": [],
      "source": [
        "#19\n",
        "df2 = pd.pivot_table(df1, index='Year', columns='Country', values='Production')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEivxdH8V80s"
      },
      "outputs": [],
      "source": [
        "#20\n",
        "df2.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLLQgABHV80s"
      },
      "outputs": [],
      "source": [
        "#21\n",
        "df2_clean = df2.dropna()/1000 # into ktonnes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGuInOwbV80s"
      },
      "outputs": [],
      "source": [
        "#22\n",
        "df2_clean.cov()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI88ik57V80s"
      },
      "outputs": [],
      "source": [
        "#23\n",
        "df2_clean.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9lC1288V80s"
      },
      "outputs": [],
      "source": [
        "#24\n",
        "im = plt.imshow(df2_clean.corr())   # vmin=-1, vmax=1\n",
        "\n",
        "# plt.xticks(ticks=[0,1,2,3,4], labels=countries)\n",
        "# plt.yticks(ticks=[0,1,2,3,4], labels=countries)\n",
        "# plt.colorbar(im)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmtIGqugV80s"
      },
      "outputs": [],
      "source": [
        "#25\n",
        "import seaborn as sns\n",
        "# Scatter matrix\n",
        "sns.pairplot(df2_clean)   # diag_kind='kde'\n",
        "plt.suptitle('Ferro-silico-manganese Production for 5 Countries', y=1.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWlZV4WfV80s"
      },
      "outputs": [],
      "source": [
        "#26\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df2_clean['Mexico'], df2_clean['Norway'], s=100, c=df2_clean.index, cmap='viridis', edgecolors='black')\n",
        "plt.colorbar(label='Year')\n",
        "plt.xlabel('Mexico Production (kt)')\n",
        "plt.ylabel('Norway Production (kt)')\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zkwONuYV80s"
      },
      "source": [
        "## 1.3 Exploratory Data Analysis\n",
        "\n",
        "*Exploratory data analysis is an attitude, a state of flexibility, \\\n",
        "a willingness to look for those things that we believe are not there, \\\n",
        "as well as those we believe to be there.*\n",
        "\n",
        "John Tukey (1915–2000)\n",
        "\n",
        "American mathematician and statistician"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccpVuqynV80s"
      },
      "source": [
        "* how does my data look like?\n",
        "* which region they cover?\n",
        "* are any data missing? NANs?\n",
        "* do we have outliers?\n",
        "* are there any patterns?\n",
        "* are data clustered around a certain point?\n",
        "* are there any correlations between variables?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YERYl7q_V80s"
      },
      "source": [
        "#### Descriptive statistics - measures of\n",
        "   * frequency (\"25% of all my data for the first attribute are less than zero\")\n",
        "   * the central tendency (**mean** ($1^{th}$ raw moment), **median**, the mode (*uni-, bi-, multi-modal*))\n",
        "   * dispersion or the variability (**variance** ($2^{th}$ central moment), **standard deviation**, **min** and **max**)\n",
        "   * measures of the shape - asymmetric data? -> **skewness** ($3^{th}$ central moment), fat tail? -> **kurtosis** ($4^{th}$ central moment)\n",
        "   * measures of the dependency of variables (**correlation**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlWvqf7BV80s"
      },
      "source": [
        "#### Data Visualization\n",
        "   * must complement the above!\n",
        "   * *Anscombe’s quartet* -> a very educative example by the statistician F. ANSCOMBE from 1973"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zETn0oXdV80s"
      },
      "outputs": [],
      "source": [
        "#27\n",
        "df = pd.read_csv(base_url + 'anscombe_quartet.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#28\n",
        "df"
      ],
      "metadata": {
        "id": "z-2LY7JnJ0Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRtwdsAoV80t"
      },
      "outputs": [],
      "source": [
        "#29\n",
        "df.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0P0GyjWV80t"
      },
      "outputs": [],
      "source": [
        "#30\n",
        "df.cov()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t20p1ECV80t"
      },
      "outputs": [],
      "source": [
        "#31\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 2), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes):\n",
        "    x = df[f'x{i+1}']\n",
        "    y = df[f'y{i+1}']\n",
        "    # Scatter plot\n",
        "    ax.plot(x, y, 'bo')\n",
        "    slope, intercept = np.polyfit(x, y, 1)\n",
        "    xfit = np.linspace(0, 20, 100)\n",
        "    ax.plot(xfit, intercept + slope * xfit, color='red', lw=2)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.grid()\n",
        "\n",
        "    # print('No. of observations of X and Y: ', len(x), len(y))\n",
        "    # print('Mean of X and Y: ', round(np.mean(x),2), round(np.mean(y),2))\n",
        "    # print('Sample variance of X and Y: %.2f %.2f' % (np.var(x,ddof=1), np.var(y,ddof=1)))\n",
        "    # print('Correlation between X and Y:  {:.2f}'.format(np.corrcoef(x,y)[0,1]))\n",
        "    # print(f'Slope and intercept: {slope:.2f}, {intercept:.2f}')\n",
        "    # print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SrODQgaV80t"
      },
      "source": [
        "* each of them has $n = 11$ observations\n",
        "* the mean of each X is exactly $\\bar{x} = 9$\n",
        "* the mean of each Y is $\\bar{y} = 7.5$\n",
        "* the sample variance of each X is exactly $\\sigma_X = 11$\n",
        "* the sample variance of each Y is $\\sigma_Y = 4.13$\n",
        "* the correlation coefficient between any pair of X and Y is Cor$(X,Y) = 0.82$\n",
        "* fitting a line to the data always gives the equation $Y = 0.5 X + 3.0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpeEdsYBV80v"
      },
      "source": [
        "### Datasaurus dozen\n",
        "\n",
        "Similar to Anscombe's quartet, the Datasaurus dozen was designed to further illustrate the importance of looking at a set of data graphically before starting to analyze according to a particular type of relationship, and the inadequacy of basic statistic properties for describing realistic data sets.\n",
        "\n",
        "![datasaurus1.jpg](https://github.com/TomasManik/FME-ZeMe-DigiWorkshop2025/blob/main/data/datasaurus1.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYq9zHo9V80w"
      },
      "source": [
        "---\n",
        "# Part 2: Data filtering and differentiation\n",
        "\n",
        "## 2.1 Solidification of Aluminium\n",
        "\n",
        "The info on the txt file:\n",
        "* first raw - the total time $T_{max}$\n",
        "* followed by rows containing voltage in (V) for evenly distributed times between $0$ and $T_{max}$\n",
        "* the conversion voltage (V) -> temperature (C) is as: $T=V\\cdot 275.5 - 79$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv6j_bT_V80w"
      },
      "outputs": [],
      "source": [
        "#1\n",
        "df = pd.read_csv(base_url + 'thermocouple_data.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylvbk95ZV80w"
      },
      "outputs": [],
      "source": [
        "#2\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SovFu7cjV80w"
      },
      "outputs": [],
      "source": [
        "#3\n",
        "data = np.loadtxt(base_url + 'thermocouple_data.txt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "data"
      ],
      "metadata": {
        "id": "VtxgRn7BhoZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsAORUBTV80w"
      },
      "outputs": [],
      "source": [
        "#5\n",
        "Tmax = data[0]\n",
        "V = data[1:]\n",
        "t = np.linspace(0, Tmax, len(V))\n",
        "T = V*275.5 - 79"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKHuoho5V80w"
      },
      "outputs": [],
      "source": [
        "#6\n",
        "plt.plot(t,T)\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Temperature (C)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2_cHr1UV80w"
      },
      "outputs": [],
      "source": [
        "#7\n",
        "dTdt = np.diff(T)/np.diff(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3UcgmjOV80w"
      },
      "outputs": [],
      "source": [
        "#8\n",
        "plt.plot(t[:-1], dTdt, '.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3WkeYTYV80w"
      },
      "source": [
        "## 2.2 Savitzky-Golay filter (1964).\n",
        "\n",
        "The Savitzky-Golay filter$^1$ is a digital filter used for smoothing noisy data while preserving the shape and height of signal features such as peaks and valleys.\n",
        "\n",
        "[1]: Savitzky, A. & Golay, M. J. E. (1964). \"Smoothing and Differentiation of Data by Simplified Least Squares Procedures.\" *Analytical Chemistry*, 36(8), 1627–1639. [https://doi.org/10.1021/ac60214a047](https://doi.org/10.1021/ac60214a047)\n",
        "\n",
        "### How it works:\n",
        "1. For each data point, a window of neighboring values is selected (window size, e.g. 5, 7, or more).\n",
        "2. A polynomial of a chosen degree (commonly 2 or 3) is least-squares fitted to all points in the window.\n",
        "3. The center point is replaced with the polynomial value at that location.\n",
        "4. The window slides over the entire data series, repeating the process for every data point.\n",
        "\n",
        "### Advantages:\n",
        "* Preserves important features (like peak width and height) better than standard moving averages.\n",
        "* Can be tuned for different types of noise and signal shapes by choosing the window size and polynomial degree.\n",
        "* Calculation of smoothed derivatives of a signal is simple (by evaluating derivatives of the fitted polynomial)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWSQ5DXkV80w"
      },
      "source": [
        "![savgol.jpg](https://github.com/TomasManik/FME-ZeMe-DigiWorkshop2025/blob/main/data/savgol.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2ZSDd7OV80w"
      },
      "source": [
        "The Savitzky-Golay smoothing equation for a cubic (third-order) polynomial over a 7-point window is:\n",
        "\n",
        "$$\\tilde{y}_n = \\frac{-2y_{n-3} + 3y_{n-2} + 6y_{n-1} + 7y_{n} + 6y_{n+1} + 3y_{n+2} - 2y_{n+3}}{21}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wiw7OyHxV80w"
      },
      "outputs": [],
      "source": [
        "#9\n",
        "# Import Savitzky-Golay filter\n",
        "from scipy.signal import savgol_filter, savgol_coeffs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpIib98rV80w"
      },
      "outputs": [],
      "source": [
        "#10\n",
        "savgol_coeffs(window_length=7, polyorder=3) #*21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzs4S6QcV80w"
      },
      "outputs": [],
      "source": [
        "#11\n",
        "WL, P = 7, 2\n",
        "\n",
        "Tf = savgol_filter(T, window_length=WL, polyorder=P)\n",
        "\n",
        "dTdt2 = savgol_filter(T, window_length=2, polyorder=1, deriv=1)\n",
        "dTdtf = savgol_filter(T, window_length=WL, polyorder=P, deriv=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mz4iUjyV80w"
      },
      "outputs": [],
      "source": [
        "#12\n",
        "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
        "ax1.plot(t,T, 'b', label='raw')\n",
        "ax1.plot(t,Tf,'r--', label='filtered')\n",
        "ax1.set_xlabel('Time')\n",
        "ax1.set_ylabel('Temperature')\n",
        "ax1.legend(loc='lower left')\n",
        "\n",
        "# ax2 = ax1.twinx() # create a secondary y-axis\n",
        "# ax2.plot(t, dTdt2, '.g', label='2-point', alpha=0.3)\n",
        "# ax2.plot(t, dTdtf, 'k', label=f'{WL}-points, {P}-poly')\n",
        "# ax2.set_ylabel('Derivative dT/dt')\n",
        "# ax2.legend(loc='lower right')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8eZmbelV80x"
      },
      "source": [
        "## 2.3 Butterworth filter (1930)\n",
        "\n",
        "The Butterworth filter$^2$ is one of the most commonly used filters in signal processing. It has **maximally flat frequency response** in the passband, making it ideal for applications where signal distortion should be minimized.\n",
        "\n",
        "[2]: Butterworth, S. (1930). \"On the Theory of Filter Amplifiers.\" *Experimental Wireless and the Wireless Engineer* 7: 536–541. [https://www.changpuak.ch/electronics/downloads/On_the_Theory_of_Filter_Amplifiers.pdf]\n",
        "\n",
        "### How it works (a low-pass version):\n",
        "* Simple design - requires only two parameters: **cutoff frequency** and **filter order**\n",
        "* Signal with frequencies below the cutoff are preserved\n",
        "* Signal with frequencies above the cutoff is progressively attenuated (how fast depends on the order)\n",
        "\n",
        "### Butterworth vs. Savitzky-Golay:\n",
        "* **Butterworth** is simple in the *frequency-domain*, but complicated in the *time-domain*\n",
        "* **Savitzky-Golay** has simple expressions in the *time-domain* (Excel-ready), but has a complicated transfer function in the *frequency-domain*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wReeAEZQV80x"
      },
      "source": [
        "![BSG.jpg](https://github.com/TomasManik/FME-ZeMe-DigiWorkshop2025/blob/main/data/BSG.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SosehUSQV80x"
      },
      "outputs": [],
      "source": [
        "#13\n",
        "# import FFT and Butterworth filter\n",
        "from scipy.fft import fft, fftfreq\n",
        "from scipy.signal import butter, sosfiltfilt, freqs, sosfreqz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8mn2qCuV80x"
      },
      "outputs": [],
      "source": [
        "#14\n",
        "data = np.loadtxt(base_url + 'thermocouple_data_noise.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4inT-m5V80x"
      },
      "outputs": [],
      "source": [
        "#15\n",
        "Tmax = 511\n",
        "V = data[1:]\n",
        "t = np.linspace(0,Tmax,len(V))\n",
        "T = V*275.5 - 79"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biWgUMkZV80x"
      },
      "outputs": [],
      "source": [
        "#16\n",
        "plt.plot(t,T)\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Temperature (C)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyeIOmO4V80x"
      },
      "outputs": [],
      "source": [
        "#17\n",
        "# Sampling frequency (Hz)\n",
        "fs = len(T)/Tmax\n",
        "print(fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlJHjGAVV80x"
      },
      "outputs": [],
      "source": [
        "#18\n",
        "# Compute FFT\n",
        "fft_T = fft(T)\n",
        "frequencies = fftfreq(len(T), d=1/fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXdamFInV80x"
      },
      "outputs": [],
      "source": [
        "#19\n",
        "# Take only positive frequencies\n",
        "freq_pos = frequencies[frequencies >= 0]\n",
        "fft_Tmagn = np.abs(fft_T[frequencies >= 0]) / len(T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL7TG26JV80x"
      },
      "outputs": [],
      "source": [
        "#20\n",
        "# plot the specturm\n",
        "plt.loglog(freq_pos, fft_Tmagn)\n",
        "plt.xlabel('Frequency (Hz)')\n",
        "plt.ylabel('Magnitude')\n",
        "plt.title('Spectrum')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVBzNoocV80x"
      },
      "source": [
        "### Create a Butterworth filter - cutoff at 0.04 Hz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3mO-F0-V80x"
      },
      "outputs": [],
      "source": [
        "#21\n",
        "sos = butter(N=10, Wn=0.04, btype='lowpass', fs=5, output='sos') # second-order section format\n",
        "T_Butter = sosfiltfilt(sos, T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MMjzNNQV80x"
      },
      "outputs": [],
      "source": [
        "#22\n",
        "plt.plot(t,T)\n",
        "plt.plot(t,T_Butter)\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Temperature (C)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMCcpH2bV80x"
      },
      "outputs": [],
      "source": [
        "#23\n",
        "dTdtf_Butter = savgol_filter(T_Butter, window_length=2, polyorder=1, deriv=1)\n",
        "\n",
        "WL = 3\n",
        "dTdtf = savgol_filter(T, window_length=WL, polyorder=2, deriv=1)\n",
        "\n",
        "plt.plot(t, dTdtf_Butter, 'r', label='Butterworth')\n",
        "plt.plot(t, dTdtf, 'g', label=f'{WL}-points')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Derivative dT/dt')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq-wE8FDV80y"
      },
      "source": [
        "---\n",
        "# Part 3: Linear and Non-Linear Regression\n",
        "\n",
        "## 3.1 Linear Regression\n",
        "\n",
        "Assume we have a set of data points $(x_i, y_i)$ for $i=1,\\dots,n$.  \n",
        "We want to approximate the relationship between $x$ and $y$ by a straight line\n",
        "\n",
        "$$\n",
        "y \\approx ax + b,\n",
        "$$\n",
        "\n",
        "where $a$ is the slope and $b$ is the intercept.  \n",
        "The parameters are chosen by the least-squares principle, i.e. by minimizing the **residual** (**objective**, **cost** or **merit** function)\n",
        "\n",
        "$$\n",
        "\\mathcal{R} = \\sum_{i=1}^n (y_i - ax_i - b)^2.\n",
        "$$\n",
        "\n",
        "The solution can be written using means, $\\bar{x}, \\bar{y}$, variance $\\operatorname{Var}(x)$ and covariance $\\operatorname{Cov}(x,y)$:\n",
        "\n",
        "$$\n",
        "a = \\frac{\\operatorname{Cov}(x,y)}{\\operatorname{Var}(x)},\n",
        "\\qquad\n",
        "b = \\bar{y} - \\bar{x}\\frac{\\operatorname{Cov}(x,y)}{\\operatorname{Var}(x)}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjZk4hAV80y"
      },
      "source": [
        "## 3.2 The Arrhenius law\n",
        "\n",
        "In many thermally activated processes in materials science and chemistry, the temperature dependence of a kinetic quantity can **often** be approximated by the Arrhenius law\n",
        "\n",
        "$$\n",
        "k(T) = k_0 \\exp\\!\\left(-\\frac{Q}{RT}\\right),\n",
        "$$\n",
        "\n",
        "where $k(T)$ is the temperature-dependent quantity, $k_0$ is a pre-exponential factor, $Q$ is an activation energy, $R$ is the gas constant, and $T$ is the absolute temperature.\n",
        "\n",
        "$k(T)$ could represent, for example:\n",
        "- a reaction rate constant,\n",
        "- a diffusion coefficient $D(T)$,\n",
        "- a mobility or interface kinetics parameter,\n",
        "- a viscosity-related rate parameter,\n",
        "- or any other thermally activated kinetic measure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRaYKSikV80y"
      },
      "outputs": [],
      "source": [
        "#1\n",
        "# Load the CSV file into Pandas DataFrame\n",
        "df = pd.read_csv(base_url + 'Arrhenius_simple.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNGfzYkfV80y"
      },
      "outputs": [],
      "source": [
        "#2\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df9d8b44"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['T'], df['k'], color='blue', marker='o')\n",
        "\n",
        "plt.xlabel('Temperature (T)')\n",
        "plt.ylabel('Rate Constant (k)')\n",
        "plt.title('Rate Constant (k) vs Temperature (T)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "#### Linearization\n",
        "\n",
        "Taking the natural logarithm gives\n",
        "\n",
        "$$\n",
        "\\ln k = \\ln k_0 - \\frac{Q}{R}\\frac{1}{T}.\n",
        "$$\n",
        "\n",
        "This is a linear model of the form\n",
        "\n",
        "$$\n",
        "y \\approx ax + b,\n",
        "$$\n",
        "\n",
        "by defining\n",
        "\n",
        "$$\n",
        "x = \\frac{1}{T},\n",
        "\\qquad\n",
        "y = \\ln k.\n",
        "$$\n",
        "\n",
        "Thus, the slope and intercept are\n",
        "\n",
        "$$\n",
        "a = -\\frac{Q}{R},\n",
        "\\qquad\n",
        "b = \\ln k_0.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "Given data points $(x_i, y_i)$ with $x_i = 1/T_i$ and $y_i = \\ln k_i$, from the least-squares minimization we get $a$ and $b$.\n",
        "\n",
        "From these, we recover the Arrhenius parameters:\n",
        "\n",
        "$$\n",
        "Q = -aR,\n",
        "\\qquad\n",
        "k_0 = \\exp(b).\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Note on interpretation\n",
        "\n",
        "A good linear fit of $\\ln k$ vs $1/T$ suggests that a single Arrhenius mechanism may be a reasonable description over the chosen temperature range. Systematic curvature often indicates that a more complex model is needed."
      ],
      "metadata": {
        "id": "_D3FWIYouTNr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LilzSTdV80y"
      },
      "outputs": [],
      "source": [
        "#3\n",
        "T = df['T'].values\n",
        "k = df['k'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPXll7U4V80y"
      },
      "outputs": [],
      "source": [
        "#4\n",
        "x, y = 1/T, np.log(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7eyqkfxV80y"
      },
      "outputs": [],
      "source": [
        "#5\n",
        "plt.scatter(x,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_5I4Y6wV80y"
      },
      "source": [
        "## 3.3 Linear regression in Python\n",
        "\n",
        "There are many ways to perform linear regression in Python:\n",
        "\n",
        "- **`linregress` from `scipy.stats`**  \n",
        "  A convenient function for simple linear regression $y = ax + b$.   \n",
        "  It works only with one predictor.   \n",
        "  Returns also statistics such as:\n",
        "  - correlation coefficient (`rvalue`)\n",
        "  - standard error of the slope (`stderr`)\n",
        "  - standard error of the intercept (`intercept_stderr`)\n",
        "\n",
        "- **`polyfit` from `numpy`**  \n",
        "  A general polynomial fitting tool; for a line - degree 1.  \n",
        "  It returns:\n",
        "  - polynomial coefficients (for degree 1: slope and intercept)  \n",
        "  - the covariance matrix of the fitted coefficients\n",
        "  - residuals\n",
        "\n",
        "- **Direct calculation from **Pandas** data**  \n",
        "  Having data stored in `DataFrame`, it’s easy to compute the least-squares slope and intercept directly using `.cov()` and `.var()`  \n",
        "\n",
        "- **`LinearRegression` from `sklearn`**  \n",
        "  A clean and scalable implementation for $y = ax + b$.  \n",
        "  It provides:\n",
        "  - attributes `coef_`, `intercept_`\n",
        "  - method `.score()` for $R^2$  \n",
        "  Advantage:\n",
        "  - naturally extends into multivariate cases\n",
        "  - integrates naturally with broader machine learning workflows.\n",
        "\n",
        "- **`curve_fit` from `scipy.optimize`**  \n",
        "  A flexible least-squares fitter for any parametric model.  \n",
        "  It need to define a function, e.g. for a line `y = f(x, a, b) = ax + b`.\n",
        "  - works for a general **nonlinear** problems  \n",
        "  It returns:\n",
        "  - best-fit parameters `popt`\n",
        "  - parameter covariance matrix `pcov`  \n",
        "\n",
        "- **`minimize` from `scipy.optimize`**  \n",
        "  The most general approach.  \n",
        "  You define any objective function you want, e.g.:\n",
        "  - sum of squares, or any other norms as L$_1$, L$_n$   \n",
        "  Advantage:\n",
        "  - works for a general **nonlinear** problems\n",
        "  - supports essentially any **user-defined residual**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzkIUYvOV80y"
      },
      "source": [
        "#### Use of `scipy.stats.linregress`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzbxaQyZV80y"
      },
      "outputs": [],
      "source": [
        "#6\n",
        "from scipy.stats import linregress\n",
        "slope, intercept, r_value, p_value, std_err = linregress(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgxygctQV80y"
      },
      "source": [
        "#### Use of `numpy.polyfit`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXS4aeRaV80y"
      },
      "outputs": [],
      "source": [
        "#7\n",
        "coeffs = np.polyfit(x, y, deg=1)\n",
        "slope, intercept = coeffs[0], coeffs[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u2-Z-a4V80y"
      },
      "source": [
        "#### Direct calculation using `.mean()`, `.cov()` `.var()` from `pandas`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czigJr5qV80y"
      },
      "outputs": [],
      "source": [
        "#8\n",
        "# Create linearized Arrhenius variables\n",
        "df['inv_T'] = 1 / df['T']\n",
        "df['log_k'] = np.log(df['k'])\n",
        "# Slope from covariance\n",
        "slope = df['inv_T'].cov(df['log_k']) / df['inv_T'].var()\n",
        "intercept = df['log_k'].mean() - slope * df['inv_T'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYaEpvSCV80y"
      },
      "source": [
        "#### Use of `sklearn.linear_model.LinearRegression`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNAqdMtkV80y"
      },
      "outputs": [],
      "source": [
        "#9\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# must reshape x from 1D to 2D\n",
        "x2D = x.reshape(len(x), 1)\n",
        "#\n",
        "model = LinearRegression()\n",
        "model.fit(x2D, y)\n",
        "#\n",
        "slope = model.coef_\n",
        "intercept = model.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPc5EJ15V80z"
      },
      "source": [
        "#### Use of `scipy.optimize.curve_fit`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkHEIzTOV80z"
      },
      "outputs": [],
      "source": [
        "#10\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "def fun_linear(x, a, b):\n",
        "    return a * x + b\n",
        "\n",
        "popt, pcov = curve_fit(fun_linear, x, y)\n",
        "slope, intercept = popt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlk5lV2qV80z"
      },
      "source": [
        "#### Use of `scipy.optimize.minimize`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuA7MbK6V80z"
      },
      "outputs": [],
      "source": [
        "#11\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def fun_residual(params, x, y):\n",
        "    slope, intercept = params\n",
        "    y_pred = slope * x + intercept\n",
        "    res = np.sum((y - y_pred)**2)\n",
        "    return res\n",
        "\n",
        "initial_guess = [0.0, 0.0]\n",
        "\n",
        "result = minimize(fun_residual, initial_guess, args=(x, y), method='BFGS')\n",
        "slope, intercept = result.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_iNkKs-V80z"
      },
      "outputs": [],
      "source": [
        "#12\n",
        "# Fitted line\n",
        "y_fit = intercept + slope * x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wAjbUm9V80z"
      },
      "outputs": [],
      "source": [
        "#13\n",
        "# Plot\n",
        "plt.scatter(x, y, c='r')\n",
        "plt.plot(x, y_fit, 'b-')\n",
        "\n",
        "plt.xlabel('1/T')\n",
        "plt.ylabel('ln(k)')\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cADNN3dBV80z"
      },
      "source": [
        "## 3.4 Confidence and prediction bands\n",
        "\n",
        "After fitting a linear model to data $(x_i, y_i)$,\n",
        "\n",
        "$$\n",
        "y \\approx ax + b,\n",
        "$$\n",
        "\n",
        "we often want to visualize the uncertainty around the fitted line $\\hat{y}(x) = \\hat{a}x + \\hat{b}$.\n",
        "\n",
        "There are two standard bands:\n",
        "\n",
        "- **Confidence band:** uncertainty in the *estimated mean response* $E[y|x]$.\n",
        "- **Prediction band:** uncertainty for a *new individual observation* at $x$ (mean uncertainty + scatter).\n",
        "\n",
        "---\n",
        "\n",
        "#### ... in the nutshell\n",
        "\n",
        "Let $n$ be the number of data points and\n",
        "\n",
        "$$\n",
        "\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i,\n",
        "\\qquad\n",
        "S_{xx} = \\sum_{i=1}^n (x_i - \\bar{x})^2.\n",
        "$$\n",
        "\n",
        "After fitting, at each $x_i$ we have residuals $e_i = y_i - \\hat{y}(x_i)$, the residual standard error $s$ is\n",
        "\n",
        "$$\n",
        "s = \\sqrt{\\frac{\\sum_{i=1}^n e_i^2}{n-2}}.\n",
        "$$\n",
        "\n",
        "The standard error of the fitted mean at $x$ is\n",
        "\n",
        "$$\n",
        "SE_{\\text{mean}}(x) = s\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{S_{xx}}}.\n",
        "$$\n",
        "\n",
        "Let $t_{1-\\alpha/2,\\,n-2}$ be the $t$-critical value obtained using the $t$-distribution with $n-2$ degrees of freedom for the desired confidence level.\n",
        "\n",
        "\n",
        "#### The $(1-\\alpha)$ confidence band\n",
        "\n",
        "$$\n",
        "\\hat{y}(x)\\ \\pm\\ t_{1-\\alpha/2,\\,n-2}\\, SE_{\\text{mean}}(x).\n",
        "$$\n",
        "\n",
        "This band is typically **narrower** and answers:  \n",
        "*\"Where is the true average trend likely to be?\"*\n",
        "\n",
        "---\n",
        "\n",
        "#### The $(1-\\alpha)$ prediction band for a new observation\n",
        "\n",
        "The standard error for a new observation at $x$ is\n",
        "\n",
        "$$\n",
        "SE_{\\text{pred}}(x) = s\\sqrt{1 + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{S_{xx}}}.\n",
        "$$\n",
        "\n",
        "Thus, the $(1-\\alpha)$ prediction band is\n",
        "\n",
        "$$\n",
        "\\hat{y}(x)\\ \\pm\\ t_{1-\\alpha/2,\\,n-2}\\, SE_{\\text{pred}}(x).\n",
        "$$\n",
        "\n",
        "This band is **wider** and answers:  \n",
        "*\"Where is a new data point likely to fall?\"*\n",
        "\n",
        "---\n",
        "\n",
        "#### Practical interpretation\n",
        "\n",
        "- The **confidence band** is about the *uncertainty of the mean line*\n",
        "- A point inside the **prediction band** is likely consistent with the model at that $x$.    \n",
        "- Both bands widen away from $\\bar{x}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2IyJ_2AV80z"
      },
      "outputs": [],
      "source": [
        "#14\n",
        "import scipy.stats as stats\n",
        "# Calculate confidence band\n",
        "\n",
        "n = len(x)\n",
        "dof = n - 2  # Degrees of freedom\n",
        "alpha = 0.05 # 95% confidence\n",
        "t_val = stats.t.ppf(1-alpha/2, dof)  # 95% confidence (two-tailed)\n",
        "\n",
        "# Residual standard error\n",
        "residuals = y - y_fit\n",
        "s = np.sqrt(np.sum(residuals**2) / dof)\n",
        "\n",
        "# Mean of x\n",
        "x_mean = np.mean(x)\n",
        "sum_sq_x = np.sum((x - x_mean)**2)\n",
        "\n",
        "# Standard error of the fit at each x\n",
        "se_fit = s * np.sqrt(1/n + (x - x_mean)**2 / sum_sq_x)\n",
        "se_pred = s * np.sqrt(1 + 1/n + (x - x_mean)**2 / sum_sq_x)\n",
        "\n",
        "# Confidence band\n",
        "margin = t_val * se_fit\n",
        "upper_band_conf = y_fit + margin\n",
        "lower_band_conf = y_fit - margin\n",
        "\n",
        "# Prediction band\n",
        "margin = t_val * se_pred\n",
        "upper_band_pred = y_fit + margin\n",
        "lower_band_pred = y_fit - margin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vUhS0jIOV80z"
      },
      "outputs": [],
      "source": [
        "#15\n",
        "# Plot\n",
        "plt.scatter(x, y, c='r', label='Exp data')\n",
        "plt.plot(x, y_fit, 'b-', lw=2, label='Linear fit')\n",
        "# Confidence band\n",
        "plt.fill_between(x, lower_band_conf, upper_band_conf, color='blue', alpha=0.4, label='95% Confidence band')\n",
        "# Prediction band\n",
        "plt.fill_between(x, lower_band_pred, upper_band_pred, color='red', alpha=0.2, label='95% Prediction band')\n",
        "\n",
        "plt.xlabel('1/T')\n",
        "plt.ylabel('ln(k)')\n",
        "plt.title('Arrhenius Plot - Linear Fit', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAW0UBzqV80z"
      },
      "source": [
        "## 3.5 Non-Linear Regression: a modified Arrhenius\n",
        "\n",
        "In many real systems, the temperature dependence of a kinetic quantity cannot be described well by a *simple* Arrhenius law with a constant prefactor. A commonly used extension is the **modified Arrhenius (Kooij) form**:\n",
        "\n",
        "$$\n",
        "k = A\\,T^n \\exp\\!\\left(-\\frac{Q}{RT}\\right),\n",
        "$$\n",
        "\n",
        "where $A$ is a pre-exponential factor, $n$ is a temperature exponent, $Q$ is an activation energy, $R$ is the gas constant, and $T$ is the absolute temperature.\n",
        "\n",
        "Taking the logarithm gives\n",
        "\n",
        "$$\n",
        "\\ln k = \\ln A + n\\ln T - \\frac{Q}{R}\\frac{1}{T}.\n",
        "$$\n",
        "\n",
        "This is not anymore a linear relationship between $\\ln k$ and $\\frac{1}{T}$.  \n",
        "However, it can be treated as a linear **multivariate** regression with predictors $\\ln T$ and $1/T$.\n",
        "\n",
        "---\n",
        "\n",
        "**1) Nonlinear least squares**\n",
        "\n",
        "by minimizing residuals\n",
        "\n",
        "$$\n",
        "\\min_{\\ln A,n,Q}\\ \\sum_{i=1}^n \\left[\\ln k_i - \\left(\\ln A + n\\ln T_i - \\frac{Q}{RT_i}\\right)\\right]^2.\n",
        "$$\n",
        "\n",
        "We can use `curve_fit` or `minimize`.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEeI_gM4V80z"
      },
      "outputs": [],
      "source": [
        "#16\n",
        "R = scipy.constants.R\n",
        "\n",
        "def ln_arrhenius(T, lnA, n, Q):\n",
        "    return lnA + n*np.log(T) - Q/(R*T)\n",
        "\n",
        "# Initial guesses\n",
        "p0 = (np.log(1e-10), 1.0, 50e3)\n",
        "\n",
        "popt, pcov = curve_fit(ln_arrhenius, T, y, p0=p0, maxfev=10000)\n",
        "\n",
        "lnA, n, Q = popt\n",
        "\n",
        "y_fit_nonlin = ln_arrhenius(T, lnA, n, Q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ5pEd1sV80z"
      },
      "outputs": [],
      "source": [
        "#17\n",
        "# Plot\n",
        "plt.scatter(x, y, c='r', label='Exp data')\n",
        "plt.plot(x, y_fit_nonlin, 'g-', label='Non-linear fit')\n",
        "plt.plot(x, y_fit, 'b--', label='Linear fit')\n",
        "plt.xlabel('1/T')\n",
        "plt.ylabel('ln(k)')\n",
        "plt.legend()\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFOHv9LVV80z"
      },
      "source": [
        "## 3.6 Confidence and prediction bands for non-linear fit\n",
        "\n",
        "For a nonlinear model fitted to data $(x_i, y_i)$,\n",
        "\n",
        "$$\n",
        "y_i = f(x_i,\\theta) + \\varepsilon_i,\n",
        "$$\n",
        "\n",
        "can we visualize uncertainty around the fitted curve?\n",
        "\n",
        "---\n",
        "\n",
        "We need to compute the parameter gradient $g(x)$ (Jacobian)\n",
        "\n",
        "$$\n",
        "g(x) = \\frac{\\partial f(x,\\theta)}{\\partial \\theta},\n",
        "$$\n",
        "\n",
        "and using the parameter covariance matrix from the fit, $\\operatorname{Cov}(\\theta)$, the standard error of the mean response can be calculated as\n",
        "\n",
        "$$\n",
        "SE_{\\text{mean}}(x) = \\sqrt{g(x)\\,\\operatorname{Cov}(\\theta)\\,g(x)^{T}}.\n",
        "$$\n",
        "\n",
        "Again, using the $t$-distribution with $(n-p)$ degrees of freedom (where $p=3$ is the number of parameters),\n",
        "\n",
        "$$\n",
        "\\hat{y}(x)\\ \\pm\\ t_{1-\\alpha/2,\\,n-p}\\, SE_{\\text{mean}}(x)\n",
        "$$\n",
        "\n",
        "gives a pointwise $(1-\\alpha)$ **confidence band**.\n",
        "\n",
        "---\n",
        "\n",
        "#### Prediction band\n",
        "\n",
        "The standard error for predicting a new individual observation at a given $x$ is\n",
        "\n",
        "$$\n",
        "SE_{\\text{pred}}(x) = \\sqrt{SE_{\\text{mean}}(x)^2 + s^2},\n",
        "$$\n",
        "\n",
        "$$\n",
        "s^2 = \\frac{\\sum_{i=1}^n (y_i - f(x_i,\\hat{\\theta}))^2}{n-p}\n",
        "$$\n",
        "\n",
        "and the $(1-\\alpha)$ **prediction band** is\n",
        "\n",
        "$$\n",
        "\\hat{y}(x)\\ \\pm\\ t_{1-\\frac{\\alpha}{2},\\,n-p}\\, SE_{\\text{pred}}(x).\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6dXWv3_V80z"
      },
      "outputs": [],
      "source": [
        "#18\n",
        "# degrees of freedom\n",
        "n_obs = len(x)\n",
        "dof = n_obs - 3\n",
        "\n",
        "# standard error\n",
        "res = y - y_fit_nonlin\n",
        "s2 = np.sum(res**2)/dof\n",
        "s = np.sqrt(s2)\n",
        "\n",
        "# Jacobian of f(T, theta) wrt theta\n",
        "def jacobian(T):\n",
        "    J = np.column_stack([np.ones_like(T), np.log(T), -1/(R*T)])\n",
        "    return J\n",
        "\n",
        "J = jacobian(T)\n",
        "var_mean = np.einsum(\"ij,jk,ik->i\", J, pcov, J)\n",
        "se_mean = np.sqrt(var_mean)\n",
        "\n",
        "# Variance for prediction adds residual variance\n",
        "se_pred = np.sqrt(var_mean + s2)\n",
        "\n",
        "# confidence interval\n",
        "alpha = 0.05\n",
        "tcrit = stats.t.ppf(1 - alpha/2, dof)\n",
        "\n",
        "# Pointwise bands in ln-space\n",
        "lower_band_conf = y_fit_nonlin - tcrit * se_mean\n",
        "upper_band_conf = y_fit_nonlin + tcrit * se_mean\n",
        "\n",
        "lower_band_pred = y_fit_nonlin - tcrit * se_pred\n",
        "upper_band_pred = y_fit_nonlin + tcrit * se_pred\n",
        "\n",
        "\n",
        "# plot\n",
        "plt.scatter(x, y, 50, 'r', label='Exp data')\n",
        "plt.plot(x, y_fit_nonlin, 'b-', lw=2, label='Non-linear fit')\n",
        "# Confidence band\n",
        "plt.fill_between(x, lower_band_conf, upper_band_conf, color='blue', alpha=0.4, label='95% Confidence band')\n",
        "# Prediction band\n",
        "plt.fill_between(x, lower_band_pred, upper_band_pred, color='red', alpha=0.2, label='95% Prediction band')\n",
        "\n",
        "plt.xlabel(\"1/T\")\n",
        "plt.ylabel(\"ln k\")\n",
        "plt.title(\"Modified Arrhenius - Non-linear fit\")\n",
        "plt.legend()\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrA50LeyV80z"
      },
      "source": [
        "## 3.7 Linear multi-variate regression\n",
        "\n",
        "In **multivariate (multiple) linear regression**, we model a response $y$ using several predictors:\n",
        "\n",
        "$$\n",
        "y_i \\approx \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}.\n",
        "$$\n",
        "\n",
        "In matrix form:\n",
        "\n",
        "$$\n",
        "\\mathbf{y} \\approx \\mathbf{X}\\boldsymbol{\\beta},\n",
        "$$\n",
        "\n",
        "where $\\mathbf{X}$ is the design matrix (first column typically ones).  \n",
        "The least-squares solution minimizes $||\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}||^2$ and is given by\n",
        "\n",
        "$$\n",
        "\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Modified Arrhenius via linear multivariate regression\n",
        "\n",
        "The **modified Arrhenius (Kooij) form** is\n",
        "\n",
        "$$\n",
        "k = A\\,T^n \\exp\\!\\left(-\\frac{Q}{RT}\\right).\n",
        "$$\n",
        "\n",
        "Taking the logarithm:\n",
        "\n",
        "$$\n",
        "\\ln k = \\ln A + n\\ln T - \\frac{Q}{R}\\frac{1}{T}.\n",
        "$$\n",
        "\n",
        "This becomes a **multivariate linear regression** problem by defining:\n",
        "\n",
        "$$\n",
        "y_i = \\ln k_i,\n",
        "\\qquad\n",
        "x_{i1} = \\ln T_i,\n",
        "\\qquad\n",
        "x_{i2} = \\frac{1}{T_i}.\n",
        "$$\n",
        "\n",
        "Hence, the model is\n",
        "\n",
        "$$\n",
        "y \\approx \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2},\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "Let's take a similar form of the Arrhenius equation,\n",
        "\n",
        "$$\n",
        "k(T,p) = A\\, p^m \\exp\\!\\left(-\\frac{Q}{RT}\\right),\n",
        "$$\n",
        "\n",
        "used widely for thermally activated processes such as **gas-phase reactions** or **surface reactions**, when the measured rate depends not only on temperature but also on the **partial pressure** of a reactive species.\n",
        "\n",
        "Taking the natural logarithm:\n",
        "\n",
        "$$\n",
        "\\ln k = \\ln A + m \\ln p - \\frac{Q}{R}\\frac{1}{T}.\n",
        "$$\n",
        "\n",
        "This is linear in the parameters when we define:\n",
        "\n",
        "$$\n",
        "y = \\ln k,\n",
        "\\qquad\n",
        "x_1 = \\ln p\n",
        "\\qquad\n",
        "x_2 = \\frac{1}{T}.\n",
        "$$\n",
        "\n",
        "So the regression model becomes:\n",
        "\n",
        "$$\n",
        "y \\approx \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2,\n",
        "$$\n",
        "\n",
        "with:\n",
        "\n",
        "$$\n",
        "\\beta_0 = \\ln A,\n",
        "\\qquad\n",
        "\\beta_1 = m,\n",
        "\\qquad\n",
        "\\beta_2 = -\\frac{Q}{R}.\n",
        "$$\n",
        "\n",
        "After fitting, we get model parameters as:\n",
        "\n",
        "$$\n",
        "A = \\exp(\\beta_0),\n",
        "\\qquad\n",
        "m = \\beta_1,\n",
        "\\qquad\n",
        "Q = -R\\beta_2.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "Construct the design matrix:\n",
        "\n",
        "$$\n",
        "\\mathbf{X} =\n",
        "\\begin{bmatrix}\n",
        "1 & \\ln T_1 & 1/T_1 \\\\\n",
        "1 & \\ln T_2 & 1/T_2 \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "1 & \\ln T_n & 1/T_n\n",
        "\\end{bmatrix},\n",
        "\\qquad\n",
        "\\mathbf{y} =\n",
        "\\begin{bmatrix}\n",
        "\\ln k_1 \\\\\n",
        "\\ln k_2 \\\\\n",
        "\\vdots \\\\\n",
        "\\ln k_n\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "and solve:\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\beta} =\n",
        "\\begin{bmatrix}\n",
        "\\beta_0 \\\\\n",
        "\\beta_1 \\\\\n",
        "\\beta_2\n",
        "\\end{bmatrix} =\n",
        "(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLhgwIGCV80z"
      },
      "outputs": [],
      "source": [
        "#19\n",
        "# Read the data from CSV\n",
        "df = pd.read_csv('arrhenius_data.csv')\n",
        "\n",
        "# Extract columns\n",
        "T = df['T'].values\n",
        "p = df['p'].values\n",
        "k = df['k'].values\n",
        "\n",
        "# Transform data for linearization\n",
        "# Model: ln(k) = ln(A) + m*ln(p) - (Q/R)*(1/T)\n",
        "y = np.log(k)\n",
        "x1 = np.log(p)\n",
        "x2 = 1/T\n",
        "\n",
        "# Stack features into matrix X\n",
        "X = np.column_stack([x1, x2])\n",
        "\n",
        "# Linear Regression\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Extract parameters\n",
        "beta0 = model.intercept_\n",
        "beta1, beta2 = model.coef_\n",
        "\n",
        "# Convert coefficients back to physical parameters\n",
        "A_hat = np.exp(beta0)   # A = exp(intercept)\n",
        "m_hat = beta1           # m = coeff of ln(p)\n",
        "Q_hat = -beta2 * R      # Q = - (coeff of 1/T) * R\n",
        "\n",
        "# Print results\n",
        "print(\"Regression Results:\")\n",
        "print(f\"Pre-exponential factor (A_hat): {A_hat:.3e}\")\n",
        "print(f\"Pressure exponent (m_hat):      {m_hat:.4f}\")\n",
        "print(f\"Activation energy (Q_hat):      {Q_hat:.2f} J/mol\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTyOnBkEV800"
      },
      "source": [
        "---\n",
        "# Part 4: 3D Plotting\n",
        "\n",
        "## 4.1  3D Scatter and Surface Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48SVP1BiV800"
      },
      "outputs": [],
      "source": [
        "#1\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Build a grid in (T, p)\n",
        "Tg = np.linspace(T.min(), T.max(), 100)\n",
        "pg = np.linspace(p.min(), p.max(), 100)\n",
        "\n",
        "TT, PP = np.meshgrid(Tg, pg)\n",
        "\n",
        "# Predict on the grid using the linear model in log-space\n",
        "X1 = np.log(PP).flatten()\n",
        "X2 = (1/TT).flatten()\n",
        "X = np.column_stack([X1, X2])\n",
        "\n",
        "lnKK = model.predict(X).reshape(TT.shape)\n",
        "KK = np.exp(lnKK)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "# 3D figure\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "# Scatter of measured/synthetic data\n",
        "scatter = ax.scatter(T, p, k, c=k, s=80, marker=\"o\", cmap='viridis', label=\"data\")\n",
        "\n",
        "# Fitted surface\n",
        "ax.plot_surface(TT, PP, KK, alpha=0.4, edgecolor='none')\n",
        "\n",
        "ax.set_xlabel(\"T (K)\")\n",
        "ax.set_ylabel(\"p (bar)\")\n",
        "ax.set_zlabel(\"k\")\n",
        "\n",
        "# Colorbar\n",
        "cbar = plt.colorbar(scatter, ax=ax)\n",
        "cbar.set_label('Rate (1/s)')\n",
        "ax.view_init(elev=20, azim=45)"
      ],
      "metadata": {
        "id": "kf1dTjEP_SOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtwJpaAyV800"
      },
      "source": [
        "## 4.2  Contour Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEH7EBo_V800"
      },
      "outputs": [],
      "source": [
        "#3\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# 1) Filled contour\n",
        "ax = axes[0]\n",
        "cf = ax.contourf(TT, PP, KK, cmap=\"RdYlGn\", levels=16)\n",
        "cs = ax.contour(TT, PP, KK, colors=\"white\", levels=16)\n",
        "ax.clabel(cs, inline=True, fontsize=10, colors='black')\n",
        "sc = ax.scatter(T, p, c=k, s=100, marker=\"o\", cmap=\"RdYlGn\", edgecolors=\"k\", linewidths=0.4)\n",
        "cbar = fig.colorbar(cf, ax=ax)\n",
        "cbar.set_label(\"k\")\n",
        "\n",
        "ax.set_xlabel(\"T (K)\")\n",
        "ax.set_ylabel(\"p (bar)\")\n",
        "\n",
        "# 2) Contour lines\n",
        "ax = axes[1]\n",
        "cs = ax.contour(TT, PP, KK, cmap=\"RdYlGn\", levels=16)\n",
        "ax.clabel(cs, inline=True, fontsize=10)\n",
        "ax.scatter(T, p, c=k, s=100, marker=\"o\", cmap=\"RdYlGn\", edgecolors=\"k\")\n",
        "ax.set_xlabel(\"T (K)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syyto2SpV800"
      },
      "source": [
        "# Part 5. Exploring Variability - Error Bars, Histograms, and Violin Plots\n",
        "\n",
        "This dataset contains measurements of **grain size**, **Vickers hardness** and **temperature** for steel samples subjected to different heat treatments:\n",
        "\n",
        "- `Sample_ID`: Unique sample identifier  \n",
        "- `Heat_Treatment`: Annealed, Quenched, Normalized, Cold_Rolled  \n",
        "- `Grain_Size_um`: Grain size in micrometers  \n",
        "- `Hardness_HV`: Vickers hardness  \n",
        "- `Measurement_Batch`: Batch number  \n",
        "- `Temperature_C`: Heat treatment temperature\n",
        "\n",
        "Our goal is to **explore variability** in grain size and related variables across treatments using `seaborn`\n",
        "- **Histograms**\n",
        "- **Kernel density estimates (KDE)** (smooth distribution estimates)\n",
        "- **Violin plots**\n",
        "- **2D-Histograms**\n",
        "- **`pairplot` and `jointplot`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnziev0PV800"
      },
      "outputs": [],
      "source": [
        "#1\n",
        "df = pd.read_csv(base_url + 'metal_grain_size_distribution.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4R7F883V800"
      },
      "outputs": [],
      "source": [
        "#2\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfNr2YQIV800"
      },
      "outputs": [],
      "source": [
        "#3\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "treatments = ['Annealed', 'Quenched', 'Normalized', 'Cold_Rolled']\n",
        "colors = ['b', 'g', 'r', 'm']\n",
        "\n",
        "for i, treat in enumerate(treatments):\n",
        "    ax = plt.subplot(2, 2, i + 1)\n",
        "\n",
        "    # get grain size for a given heat treatment\n",
        "    subset = df[df['Heat_Treatment'] == treat]['Grain_Size_um']\n",
        "\n",
        "    # Histogram\n",
        "    sns.histplot(subset, bins=20, stat='density', alpha=0.2, color=colors[i], kde=True)\n",
        "\n",
        "    # KDE\n",
        "    #sns.kdeplot(subset, color=colors[i], bw_method='scott')\n",
        "\n",
        "    ax.set_title(treat)\n",
        "    ax.set_xlabel('Grain Size (μm)')\n",
        "    ax.set_ylabel('Density')\n",
        "    ax.grid()\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGaUbgZ1V800"
      },
      "outputs": [],
      "source": [
        "#4\n",
        "sns.violinplot(data=df,\n",
        "               x='Heat_Treatment',\n",
        "               y='Grain_Size_um',\n",
        "               bw_method='scott',\n",
        "               hue='Heat_Treatment')\n",
        "\n",
        "plt.ylabel('Grain Size (μm)')\n",
        "plt.title('Grain Size Distribution by Heat Treatment')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDGxELSNV800"
      },
      "source": [
        "#### What do we see inside a Violin Plot\n",
        "\n",
        "* The White line is the Median\n",
        "\n",
        "* The Box gives Interquartile Range (IQR) - spans from 25th (Q1) to 75th (Q3) percentile\n",
        "\n",
        "* The Lines (Whiskers) define outliers - spans between Q1−1.5×IQR and Q3+1.5×IQR\n",
        "\n",
        "* The Violin Shape is given by KDE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V4iirCuV800"
      },
      "outputs": [],
      "source": [
        "#5\n",
        "df_filtered = df[df['Measurement_Batch'].isin([1, 4])]\n",
        "\n",
        "sns.violinplot(data=df_filtered,\n",
        "               x='Heat_Treatment',\n",
        "               y='Grain_Size_um',\n",
        "               hue='Measurement_Batch',\n",
        "               bw_method='scott',\n",
        "               split=True,\n",
        "               inner='quart', # 'box', 'point', 'stick'\n",
        "               palette='pastel')\n",
        "\n",
        "plt.ylabel('Grain Size (μm)')\n",
        "plt.title('rain Size Distribution by Heat Treatment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q6J5hnaV800"
      },
      "outputs": [],
      "source": [
        "#6\n",
        "fig, ax = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "# 2D histogram with color intensity\n",
        "h = ax[0].hist2d(df['Grain_Size_um'], df['Hardness_HV'], bins=25, cmap='YlOrRd')\n",
        "ax[0].set_xlabel('Grain Size (μm)')\n",
        "ax[0].set_ylabel('Hardness (HV)')\n",
        "ax[0].set_title('2D Histogram')\n",
        "ax[0].grid(alpha=0.3)\n",
        "\n",
        "# Hexbin plot\n",
        "hb = ax[1].hexbin(df['Grain_Size_um'], df['Hardness_HV'], gridsize=25, cmap='viridis')\n",
        "ax[1].set_xlabel('Grain Size (μm)')\n",
        "ax[1].set_ylabel('Hardness (HV)')\n",
        "ax[1].set_title('Hexbin Plot')\n",
        "ax[1].grid(True, alpha=0.3)\n",
        "plt.colorbar(hb, ax=ax[2], label='Count')\n",
        "\n",
        "# 2D histogram with contours\n",
        "ax[2].contour(h[1][:-1], h[2][:-1], h[0].T, cmap='viridis')\n",
        "ax[2].set_xlabel('Grain Size (μm)')\n",
        "ax[2].set_ylabel('Hardness (HV)')\n",
        "ax[2].set_title('2D Histogram with Contours')\n",
        "ax[2].grid(True, alpha=0.3)\n",
        "plt.colorbar(hb, ax=ax[1], label='Count')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZQtNq4uV800"
      },
      "outputs": [],
      "source": [
        "#7\n",
        "g = sns.jointplot(\n",
        "    data=df,\n",
        "    x='Grain_Size_um',\n",
        "    y='Hardness_HV',\n",
        "    # hue='Heat_Treatment',\n",
        "    kind='reg',          # 'reg', 'hist', 'hex', 'scatter', 'kde'\n",
        "    palette='husl',\n",
        "    # alpha=0.6,\n",
        "    # s=25,\n",
        ")\n",
        "g.set_axis_labels('Grain Size (μm)', 'Hardness (HV)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUj8EGZ3V800"
      },
      "outputs": [],
      "source": [
        "#8\n",
        "vars_to_plot = ['Grain_Size_um', 'Hardness_HV', 'Temperature_C']\n",
        "sns.set_style(\"white\")  # no grid\n",
        "\n",
        "g = sns.pairplot(\n",
        "    data=df,\n",
        "    vars=vars_to_plot,\n",
        "    hue='Heat_Treatment',\n",
        "    diag_kind='kde',\n",
        "    plot_kws={'alpha': 0.8, 's': 25, 'edgecolor': 'black'},\n",
        "    palette='husl'\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}